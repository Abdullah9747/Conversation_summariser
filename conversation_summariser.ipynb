{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EI9UFZ_gLVtF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class InputEmbeddings(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, vocab_size: int) -> None:\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (batch, seq_len) --> (batch, seq_len, d_model)\n",
        "        # Multiply by sqrt(d_model) to scale the embeddings according to the paper\n",
        "        return self.embedding(x) * math.sqrt(self.d_model)"
      ],
      "metadata": {
        "id": "-JnqEcSoLh_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.seq_len = seq_len\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # Create a matrix of shape (seq_len, d_model)\n",
        "        pe = torch.zeros(seq_len, d_model)\n",
        "        # Create a vector of shape (seq_len)\n",
        "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1) # (seq_len, 1)\n",
        "        # Create a vector of shape (d_model)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # (d_model / 2)\n",
        "        # Apply sine to even indices\n",
        "        pe[:, 0::2] = torch.sin(position * div_term) # sin(position * (10000 ** (2i / d_model))\n",
        "        # Apply cosine to odd indices\n",
        "        pe[:, 1::2] = torch.cos(position * div_term) # cos(position * (10000 ** (2i / d_model))\n",
        "        # Add a batch dimension to the positional encoding\n",
        "        pe = pe.unsqueeze(0) # (1, seq_len, d_model)\n",
        "        # Register the positional encoding as a buffer\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False) # (batch, seq_len, d_model)\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "vjCn40KbL7yL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class LayerNormalization(nn.Module):\n",
        "\n",
        "    def __init__(self, features: int, eps: float = 1e-6) -> None:\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.alpha = nn.Parameter(torch.ones(features))  # alpha is a learnable parameter\n",
        "        self.bias = nn.Parameter(torch.zeros(features))  # bias is a learnable parameter\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len, hidden_size)\n",
        "        mean = x.mean(dim=-1, keepdim=True)  # (batch, seq_len, 1), keep dimension for broadcasting\n",
        "        std = x.std(dim=-1, keepdim=True)   # (batch, seq_len, 1), keep dimension for broadcasting\n",
        "        # eps prevents division by zero or very small std values\n",
        "        return self.alpha * (x - mean) / (std + self.eps) + self.bias\n"
      ],
      "metadata": {
        "id": "Y4uD_gvDMpho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.linear_1 = nn.Linear(d_model, d_ff) # w1 and b1\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear_2 = nn.Linear(d_ff, d_model) # w2 and b2\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (batch, seq_len, d_model) --> (batch, seq_len, d_ff) --> (batch, seq_len, d_model)\n",
        "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))"
      ],
      "metadata": {
        "id": "fVFEbf40NIYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttentionBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.d_model = d_model # Embedding vector size\n",
        "        self.h = h # Number of heads\n",
        "        # Make sure d_model is divisible by h\n",
        "        assert d_model % h == 0, \"d_model is not divisible by h\"\n",
        "\n",
        "        self.d_k = d_model // h # Dimension of vector seen by each head\n",
        "        self.w_q = nn.Linear(d_model, d_model, bias=False) # Wq\n",
        "        self.w_k = nn.Linear(d_model, d_model, bias=False) # Wk\n",
        "        self.w_v = nn.Linear(d_model, d_model, bias=False) # Wv\n",
        "        self.w_o = nn.Linear(d_model, d_model, bias=False) # Wo\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    @staticmethod\n",
        "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
        "        d_k = query.shape[-1]\n",
        "        # Just apply the formula from the paper\n",
        "        # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n",
        "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "        if mask is not None:\n",
        "            # Write a very low value (indicating -inf) to the positions where mask == 0\n",
        "            attention_scores.masked_fill_(mask == 0, -1e9)\n",
        "        attention_scores = attention_scores.softmax(dim=-1) # (batch, h, seq_len, seq_len) # Apply softmax\n",
        "        if dropout is not None:\n",
        "            attention_scores = dropout(attention_scores)\n",
        "        # (batch, h, seq_len, seq_len) --> (batch, h, seq_len, d_k)\n",
        "        # return attention scores which can be used for visualization\n",
        "        return (attention_scores @ value), attention_scores\n",
        "\n",
        "    def forward(self, q, k, v, mask):\n",
        "        query = self.w_q(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "        key = self.w_k(k) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "        value = self.w_v(v) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "\n",
        "        # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n",
        "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
        "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
        "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n",
        "\n",
        "        # Calculate attention\n",
        "        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
        "\n",
        "        # Combine all the heads together\n",
        "        # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n",
        "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
        "\n",
        "        # Multiply by Wo\n",
        "        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "        return self.w_o(x)"
      ],
      "metadata": {
        "id": "uyo0nARCNPId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "-MD6C4qVNbbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example input sequences (batch_size=2, seq_len=5)\n",
        "input_sequences = [\n",
        "    [1, 2, 3, 0, 0],  # Sequence 1, padded with 0s\n",
        "    [4, 5, 6, 7, 8]   # Sequence 2, no padding\n",
        "]\n",
        "# Convert to tensor\n",
        "input_tensor = torch.tensor(input_sequences)\n",
        "# Create a padding mask where 1 indicates a valid position and 0 indicates padding\n",
        "padding_mask = (input_tensor != 0).unsqueeze(1).unsqueeze(2)\n",
        "print(\"Input Tensor:\")\n",
        "print(input_tensor)\n",
        "print(\"Padding Mask:\")\n",
        "print(padding_mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHR-IUbqNcff",
        "outputId": "63c88b91-e2ab-4932-e0a7-276696ff3afe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Tensor:\n",
            "tensor([[1, 2, 3, 0, 0],\n",
            "        [4, 5, 6, 7, 8]])\n",
            "Padding Mask:\n",
            "tensor([[[[ True,  True,  True, False, False]]],\n",
            "\n",
            "\n",
            "        [[[ True,  True,  True,  True,  True]]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualConnection(nn.Module):\n",
        "    def __init__(self, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.norm = LayerNormalization()\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        return x + self.dropout(sublayer(self.norm(x)))"
      ],
      "metadata": {
        "id": "svHSLVgDNilr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.self_attention_block = self_attention_block\n",
        "        self.feed_forward_block = feed_forward_block\n",
        "        self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(2)])\n",
        "\n",
        "    def forward(self, x, src_mask):\n",
        "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
        "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
        "        return x"
      ],
      "metadata": {
        "id": "WiTAnWSQN4m_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, layers: nn.ModuleList) -> None:\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        self.norm = LayerNormalization()\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)"
      ],
      "metadata": {
        "id": "xLZ-e9nYN7VU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout:float)->None:\n",
        "        super().__init__()\n",
        "        self.self_attention_block = self_attention_block\n",
        "        self.cross_attention_block = cross_attention_block\n",
        "        self.feed_forward_block = feed_forward_block\n",
        "        self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(3)])\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x,x,x,tgt_mask))\n",
        "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
        "        x = self.residual_connections[2](x, self.feed_forward_block)\n",
        "        return x"
      ],
      "metadata": {
        "id": "XHdOC8NlN9kl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        self.norm = LayerNormalization(features)\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
        "        return self.norm(x)"
      ],
      "metadata": {
        "id": "QCFIqvMQOAS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ProjectionLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, vocab_size) -> None:\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x) -> None:\n",
        "        # (batch, seq_len, d_model) --> (batch, seq_len, vocab_size)\n",
        "        return self.proj(x)"
      ],
      "metadata": {
        "id": "nshqS97rOFTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "\n",
        "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer) -> None:\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_embed = src_embed\n",
        "        self.tgt_embed = tgt_embed\n",
        "        self.src_pos = src_pos\n",
        "        self.tgt_pos = tgt_pos\n",
        "        self.projection_layer = projection_layer\n",
        "\n",
        "    def encode(self, src, src_mask):\n",
        "        # (batch, seq_len, d_model)\n",
        "        src = self.src_embed(src)\n",
        "        src = self.src_pos(src)\n",
        "        return self.encoder(src, src_mask)\n",
        "\n",
        "    def decode(self, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt: torch.Tensor, tgt_mask: torch.Tensor):\n",
        "        # (batch, seq_len, d_model)\n",
        "        tgt = self.tgt_embed(tgt)\n",
        "        tgt = self.tgt_pos(tgt)\n",
        "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
        "\n",
        "    def project(self, x):\n",
        "        # (batch, seq_len, vocab_size)\n",
        "        return self.projection_layer(x)"
      ],
      "metadata": {
        "id": "8r8Jgw5XOJEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int, d_model: int=512, N: int=6, h: int=8, dropout: float=0.1, d_ff: int=2048) -> Transformer:\n",
        "    # Create the embedding layers\n",
        "    src_embed = InputEmbeddings(d_model, src_vocab_size)\n",
        "    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n",
        "\n",
        "    # Create the positional encoding layers\n",
        "    src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n",
        "    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)\n",
        "\n",
        "    # Create the encoder blocks\n",
        "    encoder_blocks = []\n",
        "    for _ in range(N):\n",
        "        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
        "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
        "        encoder_block = EncoderBlock(d_model, encoder_self_attention_block, feed_forward_block, dropout)\n",
        "        encoder_blocks.append(encoder_block)\n",
        "\n",
        "    # Create the decoder blocks\n",
        "    decoder_blocks = []\n",
        "    for _ in range(N):\n",
        "        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
        "        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
        "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
        "        decoder_block = DecoderBlock(d_model, decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n",
        "        decoder_blocks.append(decoder_block)\n",
        "\n",
        "    # Create the encoder and decoder\n",
        "    encoder = Encoder(d_model, nn.ModuleList(encoder_blocks))\n",
        "    decoder = Decoder(d_model, nn.ModuleList(decoder_blocks))\n",
        "\n",
        "    # Create the projection layer\n",
        "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n",
        "\n",
        "    # Create the transformer\n",
        "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n",
        "\n",
        "    # Initialize the parameters\n",
        "    for p in transformer.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform_(p)\n",
        "\n",
        "    return transformer"
      ],
      "metadata": {
        "id": "kjkzg_1zOMZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# # Define dummy dataset (replace this with actual data)\n",
        "# class DummyDataset(Dataset):\n",
        "#     def __init__(self, src_vocab_size, tgt_vocab_size, seq_len, num_samples=1000):\n",
        "#         self.src_vocab_size = src_vocab_size\n",
        "#         self.tgt_vocab_size = tgt_vocab_size\n",
        "#         self.seq_len = seq_len\n",
        "#         self.num_samples = num_samples\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return self.num_samples\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         # Generate random source and target sequences\n",
        "#         src = torch.randint(1, self.src_vocab_size, (self.seq_len,))\n",
        "#         tgt = torch.randint(1, self.tgt_vocab_size, (self.seq_len,))\n",
        "#         return src, tgt\n",
        "\n",
        "# # Hyperparameters\n",
        "# src_vocab_size = 10000\n",
        "# tgt_vocab_size = 10000\n",
        "# src_seq_len = 20\n",
        "# tgt_seq_len = 20\n",
        "# d_model = 512\n",
        "# N = 6\n",
        "# h = 8\n",
        "# dropout = 0.1\n",
        "# d_ff = 2048\n",
        "# batch_size = 32\n",
        "# num_epochs = 10\n",
        "# learning_rate = 0.001\n",
        "\n",
        "# # Initialize dataset and dataloader\n",
        "# dataset = DummyDataset(src_vocab_size, tgt_vocab_size, src_seq_len)\n",
        "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# # Build model\n",
        "# transformer = build_transformer(src_vocab_size, tgt_vocab_size, src_seq_len, tgt_seq_len, d_model, N, h, dropout, d_ff)\n",
        "# criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding\n",
        "# optimizer = optim.Adam(transformer.parameters(), lr=learning_rate)\n",
        "\n",
        "# # Training loop\n",
        "# for epoch in range(num_epochs):\n",
        "#     transformer.train()\n",
        "#     total_loss = 0\n",
        "#     for src, tgt in dataloader:\n",
        "#         src_mask = (src != 0).unsqueeze(1).unsqueeze(2)  # Padding mask for source\n",
        "#         tgt_input = tgt[:, :-1]  # Input to the decoder (excluding <end>)\n",
        "#         tgt_output = tgt[:, 1:]  # Expected output (excluding <start>)\n",
        "#         tgt_mask = (tgt_input != 0).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "#         # Generate target sequence mask\n",
        "#         seq_len = tgt_input.size(1)\n",
        "#         tgt_sub_mask = torch.tril(torch.ones((seq_len, seq_len), device=tgt_input.device)).bool()\n",
        "#         tgt_mask = tgt_mask & tgt_sub_mask\n",
        "\n",
        "#         # Forward pass\n",
        "#         encoder_output = transformer.encode(src, src_mask)\n",
        "#         decoder_output = transformer.decode(encoder_output, src_mask, tgt_input, tgt_mask)\n",
        "#         predictions = transformer.project(decoder_output)\n",
        "\n",
        "#         # Compute loss\n",
        "#         loss = criterion(predictions.view(-1, tgt_vocab_size), tgt_output.view(-1))\n",
        "#         total_loss += loss.item()\n",
        "\n",
        "#         # Backpropagation and optimization\n",
        "#         optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#     print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(dataloader):.4f}\")\n",
        "\n",
        "# print(\"Training complete!\")\n"
      ],
      "metadata": {
        "id": "PqYueUhxOvZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data=pd.read_csv(\"samsum-test.csv\",skiprows=1,usecols=[1,2],names=[\"dialogue\",\"summary\"])\n",
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "g2iacYgWO3gV",
        "outputId": "946cc11e-b108-4f28-ebf8-861f0fd99bc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            dialogue  \\\n",
              "0  Hannah: Hey, do you have Betty's number?\\nAman...   \n",
              "1  Eric: MACHINE!\\r\\nRob: That's so gr8!\\r\\nEric:...   \n",
              "2  Lenny: Babe, can you help me with something?\\r...   \n",
              "3  Will: hey babe, what do you want for dinner to...   \n",
              "4  Ollie: Hi , are you in Warsaw\\r\\nJane: yes, ju...   \n",
              "\n",
              "                                             summary  \n",
              "0  Hannah needs Betty's number but Amanda doesn't...  \n",
              "1  Eric and Rob are going to watch a stand-up on ...  \n",
              "2  Lenny can't decide which trousers to buy. Bob ...  \n",
              "3  Emma will be home soon and she will let Will k...  \n",
              "4  Jane is in Warsaw. Ollie and Jane has a party....  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e1a13599-3ce2-4a06-a1d5-4f8ea91f869c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dialogue</th>\n",
              "      <th>summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Hannah: Hey, do you have Betty's number?\\nAman...</td>\n",
              "      <td>Hannah needs Betty's number but Amanda doesn't...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Eric: MACHINE!\\r\\nRob: That's so gr8!\\r\\nEric:...</td>\n",
              "      <td>Eric and Rob are going to watch a stand-up on ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Lenny: Babe, can you help me with something?\\r...</td>\n",
              "      <td>Lenny can't decide which trousers to buy. Bob ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Will: hey babe, what do you want for dinner to...</td>\n",
              "      <td>Emma will be home soon and she will let Will k...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Ollie: Hi , are you in Warsaw\\r\\nJane: yes, ju...</td>\n",
              "      <td>Jane is in Warsaw. Ollie and Jane has a party....</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e1a13599-3ce2-4a06-a1d5-4f8ea91f869c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e1a13599-3ce2-4a06-a1d5-4f8ea91f869c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e1a13599-3ce2-4a06-a1d5-4f8ea91f869c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ac7a5e9b-af69-450b-97e8-56a69aaed3c7\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ac7a5e9b-af69-450b-97e8-56a69aaed3c7')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ac7a5e9b-af69-450b-97e8-56a69aaed3c7 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 819,\n  \"fields\": [\n    {\n      \"column\": \"dialogue\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 819,\n        \"samples\": [\n          \"Olafur: are we doing anything for New Year's Eve?\\r\\nNathalie: I was thinking about something classy, like opera or sth like that\\r\\nZoe: how much does it cost?\\r\\nOlafur: opera is not for me\\r\\nNathalie: so what do you propose?\\r\\nNathalie: it's 100$ \\r\\nOlafur: I was thinking about partying somewhere\\r\\nNathalie: partying sounds fun, as long as it will be classy\\r\\nZoe: <file_link>\\r\\nZoe: Breakfast at Tiffany's party sounds classy\\r\\nOlafur: <file_link> \\r\\nOlafur: is it classy enough?\\r\\nNathalie: :O\\r\\nNathalie: this club is AMAZING\\r\\nZoe: whoa\\r\\nNathalie: we'll going to Soho then\\r\\nOlafur: we just need to hurry up and buy some tickets soon\\r\\nZoe: sure\",\n          \"Javier: Hey do you know any tattoo parlors over here with English speaking employees?\\r\\nJudie: Oh there's Warsaw ink\\r\\nJavier: the name sounds neat... have you had a tattoo done there?\\r\\nJudie: nope but my gf has\\r\\nJavier: got a pic?\\r\\nJudie: <file_photo>\\r\\nJavier: wow that looks amazing\\r\\nJavier: how much did she pay?\\r\\nJudie: it was a 1000\\r\\nJavier: fuck\\r\\nJavier: let me just get a tatttoo back in colombia then, thx\",\n          \"Martha: Hey, can I ask you a question?\\r\\nOphelia: Do we know each other?\\r\\nMartha: We don't, but do you mind if I ask you about the lenses from your profile picture? they are awesome and I would like to buy the similar ones\\r\\nOphelia: it's from Crazy Lenses. They have quite reasonable prices and very fast shipping.\\r\\nMartha: Thanks!!! I'll check them :)\\r\\nOphelia: No problem :)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 819,\n        \"samples\": [\n          \"Nathalie, Olafur and Zoe are planning the New Year's Eve. Nathalie wants something classy. Olafur doesn't like opera. They want to go to the Breakfast at Tiffany's party in Soho.\",\n          \"Javier was initially eager to have a tatoo done at Warsaw Ink but the price turned out to be too high. Javier decided to have a tatoo done in Colombia.\",\n          \"Martha likes Ophelia's lenses and wants to buy similar ones. Ophelia got them from Crazy Lenses.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load data\n",
        "data = pd.read_csv(\"samsum-test.csv\", skiprows=1, usecols=[1, 2], names=[\"dialogue\", \"summary\"])\n",
        "data = data.dropna()  # Ensure no missing values\n",
        "\n",
        "# Split into training and validation sets\n",
        "train_data, val_data = train_test_split(data, test_size=0.1, random_state=42)\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")  # Replace with your choice of tokenizer\n",
        "max_length = 128  # Maximum sequence length\n",
        "\n",
        "def tokenize_function(text):\n",
        "    \"\"\"Tokenizes a single text and returns input IDs.\"\"\"\n",
        "    return tokenizer(\n",
        "        text,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "# Dataset class\n",
        "class SummarizationDataset(Dataset):\n",
        "    def __init__(self, dialogues, summaries):\n",
        "        self.dialogues = dialogues.reset_index(drop=True)\n",
        "        self.summaries = summaries.reset_index(drop=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dialogues)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        dialogue = self.dialogues[idx]\n",
        "        summary = self.summaries[idx]\n",
        "\n",
        "        src = tokenize_function(dialogue)[\"input_ids\"].squeeze(0)\n",
        "        tgt = tokenize_function(summary)[\"input_ids\"].squeeze(0)\n",
        "\n",
        "        # Prepare for teacher forcing\n",
        "        tgt_input = tgt[:-1]  # Exclude last token for input\n",
        "        tgt_output = tgt[1:]  # Exclude first token for output\n",
        "\n",
        "        return {\n",
        "            \"src\": src,\n",
        "            \"tgt_input\": tgt_input,\n",
        "            \"tgt_output\": tgt_output\n",
        "        }\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = SummarizationDataset(train_data[\"dialogue\"], train_data[\"summary\"])\n",
        "val_dataset = SummarizationDataset(val_data[\"dialogue\"], val_data[\"summary\"])\n",
        "\n",
        "# DataLoader creation\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=lambda x: collate_batch(x))\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, collate_fn=lambda x: collate_batch(x))\n",
        "\n",
        "# Collate function\n",
        "def collate_batch(batch):\n",
        "    src_batch = [item[\"src\"] for item in batch]\n",
        "    tgt_input_batch = [item[\"tgt_input\"] for item in batch]\n",
        "    tgt_output_batch = [item[\"tgt_output\"] for item in batch]\n",
        "\n",
        "    src_padded = torch.nn.utils.rnn.pad_sequence(src_batch, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "    tgt_input_padded = torch.nn.utils.rnn.pad_sequence(tgt_input_batch, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "    tgt_output_padded = torch.nn.utils.rnn.pad_sequence(tgt_output_batch, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "\n",
        "    return {\n",
        "        \"src\": src_padded,\n",
        "        \"tgt_input\": tgt_input_padded,\n",
        "        \"tgt_output\": tgt_output_padded\n",
        "    }\n"
      ],
      "metadata": {
        "id": "6L_iexZiPkav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SummarizationModel(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout):\n",
        "        super(SummarizationModel, self).__init__()\n",
        "        self.transformer = nn.Transformer(d_model=d_model, nhead=nhead, num_encoder_layers=num_encoder_layers,\n",
        "                                          num_decoder_layers=num_decoder_layers, dim_feedforward=dim_feedforward,\n",
        "                                          dropout=dropout, batch_first=True)\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "        self.src_pad_idx = 0  # Assuming pad token is 0\n",
        "        self.tgt_pad_idx = 0  # Assuming pad token is 0\n",
        "\n",
        "    def forward(self, src, tgt_input, src_mask=None, tgt_mask=None):\n",
        "        # Ensure src and tgt_input are LongTensors before embedding\n",
        "        src = src.long()  # Cast to LongTensor\n",
        "        tgt_input = tgt_input.long()  # Cast to LongTensor\n",
        "\n",
        "        # Embedding the input and target sequences\n",
        "        src_emb = self.embedding(src)\n",
        "        tgt_emb = self.embedding(tgt_input)\n",
        "\n",
        "        # Pass through the transformer\n",
        "        output = self.transformer(src_emb, tgt_emb, src_mask=src_mask, tgt_mask=tgt_mask)\n",
        "\n",
        "        # Generate the output predictions\n",
        "        return self.fc_out(output)\n",
        "\n",
        "    def generate(self, src, max_length=128):\n",
        "        # Ensure src is LongTensor before embedding\n",
        "        src = src.long()  # Cast to LongTensor\n",
        "        src_emb = self.embedding(src)\n",
        "\n",
        "        # Create an empty tensor for the target sequence (used for decoding)\n",
        "        tgt = torch.ones(src.size(0), 1).to(src.device).long()  # Start token for each batch, cast to LongTensor\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            tgt_emb = self.embedding(tgt)\n",
        "            tgt_mask = self.transformer.generate_square_subsequent_mask(tgt.size(1)).to(src.device)\n",
        "\n",
        "            # Pass through the transformer\n",
        "            output = self.transformer(src_emb, tgt_emb, tgt_mask=tgt_mask)\n",
        "\n",
        "            # Get the last output token probabilities\n",
        "            logits = self.fc_out(output[:, -1, :])\n",
        "\n",
        "            # Get the next token with the highest probability\n",
        "            next_token = torch.argmax(logits, dim=-1).unsqueeze(1)\n",
        "\n",
        "            # Append the token to the target sequence\n",
        "            tgt = torch.cat((tgt, next_token), dim=1)\n",
        "\n",
        "            # Break early if we hit an end token\n",
        "            if next_token.item() == self.tgt_pad_idx:\n",
        "                break\n",
        "\n",
        "        return tgt[:, 1:]  # Return the generated tokens (remove the initial start token)\n"
      ],
      "metadata": {
        "id": "IWqR4Rp7Pq2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "# Hyperparameters\n",
        "vocab_size = tokenizer.vocab_size\n",
        "d_model = 512\n",
        "nhead = 8\n",
        "num_encoder_layers = 8\n",
        "num_decoder_layers = 8\n",
        "dim_feedforward = 2048\n",
        "dropout = 0.1\n",
        "learning_rate = 0.001\n",
        "num_epochs = 10\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Model, Loss, Optimizer\n",
        "model = SummarizationModel(vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout)\n",
        "model.to(device)  # Ensure the model is on the same device as the inputs\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        # Move data to the correct device\n",
        "        src = batch['src'].to(device)\n",
        "        tgt_input = batch['tgt_input'].to(device)\n",
        "        tgt_output = batch['tgt_output'].to(device)\n",
        "\n",
        "        # Reset gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Create masks (if needed for the transformer)\n",
        "        src_mask = None\n",
        "        tgt_mask = None\n",
        "\n",
        "        # Forward pass\n",
        "        output = model(src, tgt_input, src_mask, tgt_mask)\n",
        "\n",
        "        # Calculate loss\n",
        "        # Calculate loss\n",
        "        loss = criterion(output.reshape(-1, vocab_size), tgt_output.reshape(-1))\n",
        "\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Optimize\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Print epoch loss\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_loader)}\")\n",
        "\n",
        "print(\"Training complete!\")\n",
        "torch.save(model.state_dict(), \"transformer_model.pth\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gnIM_nCQhVk",
        "outputId": "62add189-61cb-4eff-bf9b-903468b89cdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 7.91619873046875\n",
            "Epoch 2/10, Loss: 6.436997056007385\n",
            "Epoch 3/10, Loss: 6.318034450213115\n",
            "Epoch 4/10, Loss: 6.295535922050476\n",
            "Epoch 5/10, Loss: 6.284390926361084\n",
            "Epoch 6/10, Loss: 6.282669583956401\n",
            "Epoch 7/10, Loss: 6.27705963452657\n",
            "Epoch 8/10, Loss: 6.280729293823242\n",
            "Epoch 9/10, Loss: 6.272269646326701\n",
            "Epoch 10/10, Loss: 6.2696806987126665\n",
            "Training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_score\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "def evaluate_model(model, dataloader, tokenizer):\n",
        "    model.eval()\n",
        "    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
        "    total_scores = {\"rouge1\": 0, \"rouge2\": 0, \"rougeL\": 0}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            # Assuming 'src', 'tgt_input', and 'tgt_output' are in the batch\n",
        "            src = batch['src'].to(\"cuda\")  # Input sequence\n",
        "            tgt_input = batch['tgt_input'].to(\"cuda\")  # Target input sequence\n",
        "            tgt_output = batch['tgt_output'].to(\"cuda\")  # Target output sequence\n",
        "            print(type(model))\n",
        "\n",
        "            # Generate output from the model (using the generate function)\n",
        "            output = model.generate(src, max_length=128)  # Generate predictions\n",
        "\n",
        "            # Decode predictions and references (targets)\n",
        "            for i in range(len(output)):\n",
        "                # Decode the prediction and reference (target output)\n",
        "                prediction = tokenizer.decode(output[i], skip_special_tokens=True)\n",
        "                reference = tokenizer.decode(tgt_output[i], skip_special_tokens=True)\n",
        "\n",
        "                # Print predictions and references for debugging\n",
        "                print(f\"Prediction: {prediction}\")\n",
        "                print(f\"Reference: {reference}\")\n",
        "\n",
        "                # Compute ROUGE scores\n",
        "                scores = scorer.score(reference, prediction)\n",
        "                for key in scores:\n",
        "                    total_scores[key] += scores[key].fmeasure\n",
        "\n",
        "    # Average the scores across the entire dataset\n",
        "    for key in total_scores:\n",
        "        total_scores[key] /= len(dataloader.dataset)\n",
        "\n",
        "    return total_scores\n",
        "\n",
        "# Example usage\n",
        "# Assuming `val_loader` is your validation dataloader and `model` is the trained model\n",
        "# Also assuming `tokenizer` is your tokenizer used to process the text\n",
        "scores = evaluate_model(model, val_loader, tokenizer)\n",
        "print(\"Evaluation Scores:\", scores)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 549
        },
        "id": "l8Fv5yQtQkX8",
        "outputId": "266801ea-b23e-486e-a996-08d95f49d863"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rouge_score in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.26.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.66.6)\n",
            "<class '__main__.SummarizationModel'>\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "a Tensor with 64 elements cannot be converted to Scalar",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-64cd756d7bc2>\u001b[0m in \u001b[0;36m<cell line: 44>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# Assuming `val_loader` is your validation dataloader and `model` is the trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# Also assuming `tokenizer` is your tokenizer used to process the text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Evaluation Scores:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-55-64cd756d7bc2>\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(model, dataloader, tokenizer)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;31m# Generate output from the model (using the generate function)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Generate predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;31m# Decode predictions and references (targets)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-52-35f41cc3663f>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, src, max_length)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;31m# Break early if we hit an end token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mnext_token\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtgt_pad_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: a Tensor with 64 elements cannot be converted to Scalar"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Load the trained model and tokenizer\n",
        "model_path = \"transformer_model.pth\"  # Update this if you saved the entire model\n",
        "model = build_transformer(src_vocab_size=1000, tgt_vocab_size=1000, src_seq_len=50, tgt_seq_len=50)\n",
        "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
        "model.eval()  # Set model to evaluation mode\n",
        "\n",
        "# Define a function for summarization\n",
        "def generate_summary(input_text, max_summary_length=100):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")  # Replace with your tokenizer\n",
        "    input_tokens = tokenizer.encode(input_text, return_tensors=\"pt\", truncation=True, max_length=50)\n",
        "\n",
        "    # Generate a prediction (simplified; requires model adaptation for real summarization)\n",
        "    with torch.no_grad():\n",
        "        output_tokens = model(input_tokens)\n",
        "\n",
        "    # Decode the output tokens\n",
        "    summary = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
        "    return \" \".join(summary.split()[:max_summary_length])  # Limit to max summary length\n",
        "\n",
        "# Create a text input cell\n",
        "input_text = input(\"Enter the text you want summarized: \")\n",
        "\n",
        "# Generate and display the summary\n",
        "summary = generate_summary(input_text)\n",
        "print(\"\\nSummary (max 100 words):\")\n",
        "print(summary)\n"
      ],
      "metadata": {
        "id": "G2m2B9zmWFRD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}